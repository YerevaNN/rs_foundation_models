{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cfdf34-bdea-4a1e-b688-87e9781977a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import change_detection_pytorch as cdp\n",
    "from change_detection_pytorch.datasets import LEVIR_CD_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from change_detection_pytorch.datasets import ChangeDetectionDataModule\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1c0c42-42b1-41a3-a8dc-acd39ea38ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    experiment_name: str = 'tmp'\n",
    "    backbone: str = 'Swin-B'\n",
    "    encoder_weights: str = 'geopile'\n",
    "    encoder_depth: int = 12\n",
    "    dataset_name: str = 'OSCD'\n",
    "    dataset_path: str = '/mnt/sxtn/aerial/change/OSCD/'\n",
    "    fusion: str = 'diff'\n",
    "    scale: str = None\n",
    "    tile_size: int = 192\n",
    "    mode: str = 'vanilla'\n",
    "    batch_size: int = 116 // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f35be00-fedf-4810-96fa-5ce014105381",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8be844-3c54-436e-98b7-583a6d9e381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = ['1x', '2x', '4x', '8x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be96adbe-e337-4bf5-8624-9b91f7b1e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [\n",
    "    '/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6198c21f-16e9-4135-acae-7366d0c27d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_bitwise(y_true, y_pred):\n",
    "    TP = np.bitwise_and(y_true, y_pred).sum()\n",
    "    FP = np.bitwise_and(y_pred, np.logical_not(y_true)).sum()\n",
    "    FN = np.bitwise_and(np.logical_not(y_pred), y_true).sum()\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-10)\n",
    "    recall = TP / (TP + FN + 1e-10)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f6672f-bd51-4fd1-aee0-2b7b063d32a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth']\n"
     ]
    }
   ],
   "source": [
    "print(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84414cf-471c-459d-9d9b-6bb2db30dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from change_detection_pytorch.base.modules import Activation\n",
    "from change_detection_pytorch.utils import base\n",
    "from change_detection_pytorch.utils import functional as F\n",
    "\n",
    "class CustomMetric(base.Metric):\n",
    "            __name__ = 'custom'\n",
    "        \n",
    "            def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.eps = eps\n",
    "                self.threshold = threshold\n",
    "                self.activation = Activation(activation)\n",
    "                self.ignore_channels = ignore_channels\n",
    "        \n",
    "            def forward(self, y_pr, y_gt):\n",
    "                y_pr = self.activation(y_pr)\n",
    "                data['p'] = np.concatenate([data['p'], y_pr.cpu().numpy().astype('uint8')])\n",
    "                data['t'] = np.concatenate([data['t'], y_gt.cpu().numpy().astype('uint8')])\n",
    "                \n",
    "                fscores = torch.tensor([F.f_score(p, g) for p, g in zip(y_pr, y_gt)])\n",
    "                # plt.figure(figsize=(4,2))\n",
    "                # plt.imshow((y_pr[0]*2+y_gt[0]).cpu().numpy(), cmap='nipy_spectral', vmax=4)\n",
    "                # plt.title(f\"F-score={fscores[0]:.3f}\")\n",
    "                # print(\"\\n\", y_pr.shape, y_gt.shape)\n",
    "                # print((y_pr*y_gt).sum(), y_pr.sum(), y_gt.sum())\n",
    "                return fscores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2171c99a-4063-4b3b-9c2c-7b82643b7d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args(experiment_name='tmp', backbone='ibot-B', encoder_weights='million_aid', encoder_depth=12, dataset_name='LEVIR_CD', dataset_path='/mnt/sxtn/aerial/change/LEVIR_CD', fusion='diff', scale=None, tile_size=256, mode='vanilla', batch_size=1)\n",
      "Pretrained weights found at /nfs/go/mnt/bolbol/alla/checkpoints/ibot_checkpoints/vitb_16/checkpoint0080.pth and loaded with msg: _IncompatibleKeys(missing_keys=['neck.lateral_convs.0.conv.weight', 'neck.lateral_convs.0.conv.bias', 'neck.lateral_convs.1.conv.weight', 'neck.lateral_convs.1.conv.bias', 'neck.lateral_convs.2.conv.weight', 'neck.lateral_convs.2.conv.bias', 'neck.lateral_convs.3.conv.weight', 'neck.lateral_convs.3.conv.bias', 'neck.convs.0.conv.weight', 'neck.convs.0.conv.bias', 'neck.convs.1.conv.weight', 'neck.convs.1.conv.bias', 'neck.convs.2.conv.weight', 'neck.convs.2.conv.bias', 'neck.convs.3.conv.weight', 'neck.convs.3.conv.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v', 'head.last_layer2.weight_g', 'head.last_layer2.weight_v'])\n",
      "LEVIR LEVIR_CD\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2048/2048 [03:09<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:09, 209.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth 1x\n",
      "0.3631313087976997 0.9055349360167491\n",
      "LEVIR LEVIR_CD\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2048/2048 [02:31<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:09, 209.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth 2x\n",
      "0.3427380594544853 0.8789257288189157\n",
      "LEVIR LEVIR_CD\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2048/2048 [02:35<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:10, 200.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth 4x\n",
      "0.14599632198083615 0.5188903576210128\n",
      "LEVIR LEVIR_CD\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2048/2048 [02:10<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:10, 193.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth 8x\n",
      "0.014100647567644568 0.037060508633233326\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_path in checkpoints:\n",
    "    results[checkpoint_path] = {}\n",
    "    args = Args()  # Create an instance with default values\n",
    "\n",
    "    if 'ibot' in checkpoint_path:\n",
    "        args.backbone = 'ibot-B'\n",
    "        args.encoder_weights = 'million_aid'\n",
    "        args.encoder_depth = 12\n",
    "    elif 'satlas' in checkpoint_path:\n",
    "        args.backbone = 'Swin-B'\n",
    "        args.encoder_weights = 'satlas'\n",
    "        args.encoder_depth = 12\n",
    "    elif 'cmid' in checkpoint_path:\n",
    "        args.backbone = 'Swin-B'\n",
    "        args.encoder_weights = 'cmid'\n",
    "        args.encoder_depth = 12\n",
    "    \n",
    "    if 'cdd' in checkpoint_path:\n",
    "        args.dataset_name = 'CDD'\n",
    "        args.dataset_path = '/mnt/sxtn/aerial/change/CDD/Real/subset/'\n",
    "        args.batch_size = 32\n",
    "        args.tile_size = 256 # it doesn't use\n",
    "    elif 'levir' in checkpoint_path:\n",
    "        args.dataset_name = 'LEVIR_CD'\n",
    "        args.dataset_path = '/mnt/sxtn/aerial/change/LEVIR_CD'\n",
    "        args.batch_size = 1\n",
    "        args.tile_size = 256 # it doesn't use\n",
    "        \n",
    "    print(args)\n",
    "            \n",
    "    DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # args.fusion = 'concat' # comment this for other models\n",
    "    \n",
    "    model = cdp.UPerNet(\n",
    "        encoder_depth=args.encoder_depth,\n",
    "        encoder_name=args.backbone, # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=args.encoder_weights, # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3, # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2, # model output channels (number of classes in your datasets)\n",
    "        siam_encoder=True, # whether to use a siamese encoder\n",
    "        fusion_form=args.fusion, # the form of fusing features from two branches. e.g. concat, sum, diff, or abs_diff.\n",
    "    )\n",
    "\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint.state_dict())\n",
    "\n",
    "    for scale in scales:\n",
    "        if scale != '1x':\n",
    "            args.scale = scale\n",
    "            args.mode = 'wo_train_aug'\n",
    "        if 'cdd' in args.dataset_name.lower():\n",
    "            print('CDD', args.dataset_name)\n",
    "            valid_dataset = LEVIR_CD_Dataset(f'{args.dataset_path}/test',\n",
    "                                            sub_dir_1='A',\n",
    "                                            sub_dir_2='B' if scale == '1x' else f'B_{scale}',\n",
    "                                            img_suffix='.jpg',\n",
    "                                            ann_dir=f'{args.dataset_path}/test/OUT',\n",
    "                                            debug=False,\n",
    "                                            seg_map_suffix='.jpg')\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "            data = {\n",
    "                'p': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                't': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                'f': []\n",
    "            }\n",
    "\n",
    "\n",
    "        elif 'levir' in args.dataset_name.lower():\n",
    "            print('LEVIR', args.dataset_name)\n",
    "            valid_dataset = LEVIR_CD_Dataset(f'{args.dataset_path}/test',\n",
    "                                            sub_dir_1='A_cut',\n",
    "                                            sub_dir_2='B_cut' if scale == '1x' else f'B_cut_{scale}',\n",
    "                                            img_suffix='.png',\n",
    "                                            # ann_dir=f'{args.dataset_path}/test/OUT',\n",
    "                                            ann_dir=f'{args.dataset_path}/test/OUT_cut',\n",
    "                                            debug=False,\n",
    "                                            test_mode=True,\n",
    "                                            seg_map_suffix='.png')\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "            data = {\n",
    "                'p': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                't': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                'f': []\n",
    "            }\n",
    "\n",
    "\n",
    "        elif 'oscd' in args.dataset_name.lower():\n",
    "            print('oscd', args.dataset_name)\n",
    "            datamodule = ChangeDetectionDataModule(args.dataset_path, patch_size=args.tile_size, mode=args.mode, scale=args.scale, batch_size=args.batch_size)\n",
    "            datamodule.setup()\n",
    "            \n",
    "            valid_loader = datamodule.val_dataloader()\n",
    "            \n",
    "            data = {\n",
    "                'p': np.empty((0, 192, 192), dtype='uint8'),\n",
    "                't': np.empty((0, 192, 192), dtype='uint8'),\n",
    "                'f': []\n",
    "            }\n",
    "            \n",
    "        loss = cdp.utils.losses.CrossEntropyLoss()\n",
    "        \n",
    "        our_metrics = [\n",
    "            cdp.utils.metrics.Fscore(activation='argmax2d'),\n",
    "            cdp.utils.metrics.Precision(activation='argmax2d'),\n",
    "            cdp.utils.metrics.Recall(activation='argmax2d'),\n",
    "            CustomMetric(activation='argmax2d'),\n",
    "        ]\n",
    "        \n",
    "        valid_epoch = cdp.utils.train.ValidEpoch(\n",
    "            model,\n",
    "            loss=loss,\n",
    "            metrics=our_metrics,\n",
    "            device=DEVICE,\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "                \n",
    "        if 'cdd' in args.dataset_name.lower() or 'levir' in args.dataset_name.lower():\n",
    "            fscores = []\n",
    "            maps_t = []\n",
    "            maps_p = []\n",
    "            for p, t in tqdm(zip(data['p'], data['t'])):\n",
    "                if p.sum() + t.sum() == 0:\n",
    "                    fscores.append(0)\n",
    "                else:\n",
    "                    f1_real = metrics.f1_score(t.flatten(), p.flatten())\n",
    "                    # f1_ours = f1_bitwise(t.flatten(), p.flatten())\n",
    "                    fscores.append(\n",
    "                        f1_real\n",
    "                    )\n",
    "                maps_t.append(t)\n",
    "                maps_p.append(p)\n",
    "            macro_f1 = np.mean(fscores)\n",
    "            maps_t = np.vstack(maps_t)\n",
    "            maps_p = np.vstack(maps_p)\n",
    "            print(maps_t.shape, maps_p.shape)\n",
    "\n",
    "            print(maps_t)\n",
    "            print(maps_t.dtype)\n",
    "            micro_f1 = f1_bitwise(maps_t, maps_p)\n",
    "            maps = {'t':maps_t, 'p':maps_p}\n",
    "        else:\n",
    "            data['f'] = [y for x in valid_logs['filenames'] for y in x]\n",
    "            cities = []\n",
    "            coords = []\n",
    "            for name in data['f']:\n",
    "                name = name.split('/')[-1]\n",
    "                _parts = name.split('_')\n",
    "                city = '_'.join(_parts[:-1])\n",
    "                coord = [int(t) for t in _parts[-1][1:-1].split(', ')]\n",
    "                cities.append(city)\n",
    "                coords.append(coord)\n",
    "            unique_cities = set(cities)\n",
    "            maps = {city: {\n",
    "                't': np.zeros((1000, 1000)),\n",
    "                'p': np.zeros((1000, 1000)),\n",
    "            } for city in unique_cities}\n",
    "            for city, coord, p, t in zip(cities, coords, data['p'], data['t']):\n",
    "                x1,y1,x2,y2 = coord\n",
    "                maps[city]['t'][y1:y2,x1:x2] = t\n",
    "                maps[city]['p'][y1:y2,x1:x2] = p\n",
    "            for city in tqdm(maps.keys()):\n",
    "                maps[city]['fscore'] = metrics.f1_score(maps[city]['t'].flatten(), maps[city]['p'].flatten())\n",
    "            micro_f1 = metrics.f1_score(\n",
    "                np.concatenate([maps[city]['t'].flatten() for city in maps]),\n",
    "                np.concatenate([maps[city]['p'].flatten() for city in maps]), \n",
    "            )\n",
    "            macro_f1 = np.mean([maps[city]['fscore'] for city in maps]) \n",
    "            \n",
    "        print(checkpoint_path, scale)\n",
    "        print(macro_f1, micro_f1)\n",
    "    \n",
    "        results[checkpoint_path][scale] = {\n",
    "            'maps': maps,\n",
    "            'micro_f1': micro_f1,\n",
    "            'macro_f1': macro_f1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c63fd6-4241-4126-bfaf-479aff905247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth\n",
      "1x macro F1 = 0.363  micro-F1 = 0.906\n",
      "2x macro F1 = 0.343  micro-F1 = 0.879\n",
      "4x macro F1 = 0.146  micro-F1 = 0.519\n",
      "8x macro F1 = 0.014  micro-F1 = 0.037\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)\n",
    "    for scale in scales:\n",
    "        print(f\"{scale} macro F1 = {results[checkpoint][scale]['macro_f1']:.3f}  micro-F1 = {results[checkpoint][scale]['micro_f1']:.3f}\")\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59964303-3006-41fa-939f-eac981d3f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_colorjitter/best_model.pth\n",
      ",0.037,0.519,0.879,0.906 \n"
     ]
    }
   ],
   "source": [
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)\n",
    "    for scale in ['8x', '4x', '2x', '1x']:\n",
    "        print(f\",{results[checkpoint][scale]['micro_f1']:.3f}\", end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a27a24-3462-4cf6-bfaf-14e715e61f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f47d6-d79e-4bf3-8db7-d360b361f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7a7ae-b276-47cd-bd29-2a993aa4deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdad231-eef7-4eb1-b2f5-a7b77953eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.unsqueeze(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956ef11-08f9-41d1-a3f8-4aadb77a4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881a746-9fdd-4610-8533-9bc77e0c1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a7ae3-cf90-4380-82d7-db2506a7b11d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
