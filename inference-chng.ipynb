{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cfdf34-bdea-4a1e-b688-87e9781977a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import change_detection_pytorch as cdp\n",
    "from change_detection_pytorch.datasets import LEVIR_CD_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from change_detection_pytorch.datasets import ChangeDetectionDataModule\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6aa42a-4504-4ae2-ada9-c105d678a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR']=\"localhost\"\n",
    "os.environ['MASTER_PORT']=\"12346\"\n",
    "\n",
    "dist.init_process_group(backend='nccl', init_method='env://')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1c0c42-42b1-41a3-a8dc-acd39ea38ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    experiment_name: str = 'tmp'\n",
    "    backbone: str = 'Swin-B'\n",
    "    encoder_weights: str = 'geopile'\n",
    "    encoder_depth: int = 12\n",
    "    dataset_name: str = 'OSCD'\n",
    "    dataset_path: str = '/mnt/sxtn/aerial/change/OSCD/'\n",
    "    fusion: str = 'diff'\n",
    "    scale: str = None\n",
    "    tile_size: int = 192\n",
    "    mode: str = 'vanilla'\n",
    "    batch_size: int = 116 // 4\n",
    "    load_decoder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f35be00-fedf-4810-96fa-5ce014105381",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8be844-3c54-436e-98b7-583a6d9e381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = ['1x', '2x', '4x', '8x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be96adbe-e337-4bf5-8624-9b91f7b1e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [\n",
    "    '/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_scale_aug/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_overlap_seg_head_e150_lr3e-5/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/cdd_ibot_overlap_seg_head_e150_lr3e-5/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/cdd_ibot_overlap_seg_head/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_overlap_seg_head/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/cdd_ibot_overlap_seg_head_e150/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_overlap_seg_head_lr3e-5/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_overlap_seg_head_e150/best_model.pth',\n",
    "    # '/auto/home/ani/change_detection.pytorch/checkpoints/cdd_ibot_overlap_seg_head_lr3e-5/best_model.pth',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6198c21f-16e9-4135-acae-7366d0c27d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_bitwise(y_true, y_pred):\n",
    "    TP = np.bitwise_and(y_true, y_pred).sum()\n",
    "    FP = np.bitwise_and(y_pred, np.logical_not(y_true)).sum()\n",
    "    FN = np.bitwise_and(np.logical_not(y_pred), y_true).sum()\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-10)\n",
    "    recall = TP / (TP + FN + 1e-10)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f6672f-bd51-4fd1-aee0-2b7b063d32a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_scale_aug/best_model.pth']\n"
     ]
    }
   ],
   "source": [
    "print(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84414cf-471c-459d-9d9b-6bb2db30dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from change_detection_pytorch.base.modules import Activation\n",
    "from change_detection_pytorch.utils import base\n",
    "from change_detection_pytorch.utils import functional as F\n",
    "\n",
    "class CustomMetric(base.Metric):\n",
    "            __name__ = 'custom'\n",
    "        \n",
    "            def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.eps = eps\n",
    "                self.threshold = threshold\n",
    "                self.activation = Activation(activation)\n",
    "                self.ignore_channels = ignore_channels\n",
    "        \n",
    "            def forward(self, y_pr, y_gt):\n",
    "                y_pr = self.activation(y_pr)\n",
    "                data['p'] = np.concatenate([data['p'], y_pr.cpu().numpy().astype('uint8')])\n",
    "                data['t'] = np.concatenate([data['t'], y_gt.cpu().numpy().astype('uint8')])\n",
    "                \n",
    "                fscores = torch.tensor([F.f_score(p, g) for p, g in zip(y_pr, y_gt)])\n",
    "                # plt.figure(figsize=(4,2))\n",
    "                # plt.imshow((y_pr[0]*2+y_gt[0]).cpu().numpy(), cmap='nipy_spectral', vmax=4)\n",
    "                # plt.title(f\"F-score={fscores[0]:.3f}\")\n",
    "                # print(\"\\n\", y_pr.shape, y_gt.shape)\n",
    "                # print((y_pr*y_gt).sum(), y_pr.sum(), y_gt.sum())\n",
    "                return fscores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2171c99a-4063-4b3b-9c2c-7b82643b7d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args(experiment_name='tmp', backbone='ibot-B', encoder_weights='million_aid', encoder_depth=12, dataset_name='LEVIR_CD', dataset_path='/nfs/ap/mnt/sxtn/aerial/change/LEVIR_CD', fusion='diff', scale=None, tile_size=256, mode='vanilla', batch_size=64)\n",
      "Pretrained weights found at /nfs/go/mnt/bolbol/alla/checkpoints/ibot_checkpoints/vitb_16/checkpoint0080.pth and loaded with msg: _IncompatibleKeys(missing_keys=['neck.lateral_convs.0.conv.weight', 'neck.lateral_convs.0.conv.bias', 'neck.lateral_convs.1.conv.weight', 'neck.lateral_convs.1.conv.bias', 'neck.lateral_convs.2.conv.weight', 'neck.lateral_convs.2.conv.bias', 'neck.lateral_convs.3.conv.weight', 'neck.lateral_convs.3.conv.bias', 'neck.convs.0.conv.weight', 'neck.convs.0.conv.bias', 'neck.convs.1.conv.weight', 'neck.convs.1.conv.bias', 'neck.convs.2.conv.weight', 'neck.convs.2.conv.bias', 'neck.convs.3.conv.weight', 'neck.convs.3.conv.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v', 'head.last_layer2.weight_g', 'head.last_layer2.weight_v'])\n",
      "LEVIR LEVIR_CD {'/nfs/ap/mnt/sxtn/aerial/change/LEVIR_CD'}\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 32/32 [01:42<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:09, 221.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_scale_aug/best_model.pth 1x\n",
      "0.35452911298449163 0.8977437375153011\n",
      "LEVIR LEVIR_CD {'/nfs/ap/mnt/sxtn/aerial/change/LEVIR_CD'}\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:59<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:09, 222.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_scale_aug/best_model.pth 2x\n",
      "0.3537505679389352 0.8971167305964438\n",
      "LEVIR LEVIR_CD {'/nfs/ap/mnt/sxtn/aerial/change/LEVIR_CD'}\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:09, 224.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_scale_aug/best_model.pth 4x\n",
      "0.3515977558844875 0.8923869670706764\n",
      "LEVIR LEVIR_CD {'/nfs/ap/mnt/sxtn/aerial/change/LEVIR_CD'}\n",
      "Loaded 2048 images\n",
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:49<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [00:09, 225.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524288, 256) (524288, 256)\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "uint8\n",
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_scale_aug/best_model.pth 8x\n",
      "0.3418928398340948 0.8748598267186741\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_path in checkpoints:\n",
    "    results[checkpoint_path] = {}\n",
    "    args = Args()  # Create an instance with default values\n",
    "\n",
    "    if 'ibot' in checkpoint_path:\n",
    "        if 'overlap' in checkpoint_path:\n",
    "            args.backbone = 'overlap_ibot-B'\n",
    "            args.encoder_weights = 'million_aid_overlap'\n",
    "            args.encoder_depth = 12\n",
    "            args.fusion = 'concat' \n",
    "            args.load_decoder = True \n",
    "\n",
    "        else:\n",
    "            args.backbone = 'ibot-B'\n",
    "            args.encoder_weights = 'million_aid'\n",
    "            args.encoder_depth = 12\n",
    "            \n",
    "    elif 'satlas' in checkpoint_path:\n",
    "        args.backbone = 'Swin-B'\n",
    "        args.encoder_weights = 'satlas'\n",
    "        args.encoder_depth = 12\n",
    "    elif 'cmid' in checkpoint_path:\n",
    "        args.backbone = 'Swin-B'\n",
    "        args.encoder_weights = 'cmid'\n",
    "        args.encoder_depth = 12\n",
    "    \n",
    "    if 'cdd' in checkpoint_path:\n",
    "        args.dataset_name = 'CDD'\n",
    "        args.dataset_path = '/mnt/sxtn/aerial/change/CDD/Real/subset/'\n",
    "        args.batch_size = 32\n",
    "        args.tile_size = 256 # it doesn't use\n",
    "    elif 'levir' in checkpoint_path:\n",
    "        args.dataset_name = 'LEVIR_CD'\n",
    "        args.dataset_path = '/nfs/ap/mnt/sxtn/aerial/change/LEVIR_CD'\n",
    "        args.batch_size = 64\n",
    "        args.tile_size = 256 # it doesn't use\n",
    "        \n",
    "    print(args)\n",
    "                            \n",
    "    DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # args.fusion = 'abs_diff'\n",
    "    \n",
    "    model = cdp.UPerNet(\n",
    "        encoder_depth=args.encoder_depth,\n",
    "        encoder_name=args.backbone, # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=args.encoder_weights, # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3, # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2, # model output channels (number of classes in your datasets)\n",
    "        siam_encoder=True, # whether to use a siamese encoder\n",
    "        fusion_form=args.fusion, # the form of fusing features from two branches. e.g. concat, sum, diff, or abs_diff.\n",
    "        pretrained = args.load_decoder\n",
    "    )\n",
    "    model.to('cuda:{}'.format(dist.get_rank()))\n",
    "    model = DDP(model)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cuda:{}'.format(dist.get_rank()))\n",
    "    \n",
    "    model.load_state_dict(checkpoint.state_dict())\n",
    "\n",
    "    for scale in scales:\n",
    "        if scale != '1x':\n",
    "            args.scale = scale\n",
    "            args.mode = 'wo_train_aug'\n",
    "        if 'cdd' in args.dataset_name.lower():\n",
    "            print('CDD', args.dataset_name)\n",
    "            valid_dataset = LEVIR_CD_Dataset(f'{args.dataset_path}/test',\n",
    "                                            sub_dir_1='A',\n",
    "                                            sub_dir_2='B' if scale == '1x' else f'B_{scale}',\n",
    "                                            img_suffix='.jpg',\n",
    "                                            ann_dir=f'{args.dataset_path}/test/OUT',\n",
    "                                            debug=False,\n",
    "                                            seg_map_suffix='.jpg')\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "            data = {\n",
    "                'p': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                't': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                'f': []\n",
    "            }\n",
    "\n",
    "\n",
    "        elif 'levir' in args.dataset_name.lower():\n",
    "            print('LEVIR', args.dataset_name, {args.dataset_path})\n",
    "            valid_dataset = LEVIR_CD_Dataset(f'{args.dataset_path}/test',\n",
    "                                            sub_dir_1='A_cut',\n",
    "                                            sub_dir_2='B_cut' if scale == '1x' else f'B_cut_{scale}',\n",
    "                                            img_suffix='.png',\n",
    "                                            # ann_dir=f'{args.dataset_path}/test/OUT',\n",
    "                                            ann_dir=f'{args.dataset_path}/test/OUT_cut',\n",
    "                                            debug=False,\n",
    "                                            test_mode=True,\n",
    "                                            seg_map_suffix='.png')\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "            data = {\n",
    "                'p': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                't': np.empty((0, 256, 256), dtype='uint8'),\n",
    "                'f': []\n",
    "            }\n",
    "\n",
    "\n",
    "        elif 'oscd' in args.dataset_name.lower():\n",
    "            print('oscd', args.dataset_name)\n",
    "            datamodule = ChangeDetectionDataModule(args.dataset_path, bands = ['B04', 'B02', 'B05'], patch_size=args.tile_size, mode=args.mode, scale=args.scale, batch_size=args.batch_size)\n",
    "            datamodule.setup()\n",
    "            \n",
    "            valid_loader = datamodule.val_dataloader()\n",
    "            \n",
    "            data = {\n",
    "                'p': np.empty((0, 192, 192), dtype='uint8'),\n",
    "                't': np.empty((0, 192, 192), dtype='uint8'),\n",
    "                'f': []\n",
    "            }\n",
    "            \n",
    "        loss = cdp.utils.losses.CrossEntropyLoss()\n",
    "        \n",
    "        our_metrics = [\n",
    "            cdp.utils.metrics.Fscore(activation='argmax2d'),\n",
    "            cdp.utils.metrics.Precision(activation='argmax2d'),\n",
    "            cdp.utils.metrics.Recall(activation='argmax2d'),\n",
    "            CustomMetric(activation='argmax2d'),\n",
    "        ]\n",
    "        \n",
    "        valid_epoch = cdp.utils.train.ValidEpoch(\n",
    "            model,\n",
    "            loss=loss,\n",
    "            metrics=our_metrics,\n",
    "            device=DEVICE,\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "                \n",
    "        if 'cdd' in args.dataset_name.lower() or 'levir' in args.dataset_name.lower():\n",
    "            fscores = []\n",
    "            maps_t = []\n",
    "            maps_p = []\n",
    "            for p, t in tqdm(zip(data['p'], data['t'])):\n",
    "                if p.sum() + t.sum() == 0:\n",
    "                    fscores.append(0)\n",
    "                else:\n",
    "                    f1_real = metrics.f1_score(t.flatten(), p.flatten())\n",
    "                    # f1_ours = f1_bitwise(t.flatten(), p.flatten())\n",
    "                    fscores.append(\n",
    "                        f1_real\n",
    "                    )\n",
    "                maps_t.append(t)\n",
    "                maps_p.append(p)\n",
    "            macro_f1 = np.mean(fscores)\n",
    "            maps_t = np.vstack(maps_t)\n",
    "            maps_p = np.vstack(maps_p)\n",
    "            print(maps_t.shape, maps_p.shape)\n",
    "\n",
    "            print(maps_t)\n",
    "            print(maps_t.dtype)\n",
    "            micro_f1 = f1_bitwise(maps_t, maps_p)\n",
    "            maps = {'t':maps_t, 'p':maps_p}\n",
    "        else:\n",
    "            data['f'] = [y for x in valid_logs['filenames'] for y in x]\n",
    "            cities = []\n",
    "            coords = []\n",
    "            for name in data['f']:\n",
    "                name = name.split('/')[-1]\n",
    "                _parts = name.split('_')\n",
    "                city = '_'.join(_parts[:-1])\n",
    "                coord = [int(t) for t in _parts[-1][1:-1].split(', ')]\n",
    "                cities.append(city)\n",
    "                coords.append(coord)\n",
    "            unique_cities = set(cities)\n",
    "            maps = {city: {\n",
    "                't': np.zeros((1000, 1000)),\n",
    "                'p': np.zeros((1000, 1000)),\n",
    "            } for city in unique_cities}\n",
    "            for city, coord, p, t in zip(cities, coords, data['p'], data['t']):\n",
    "                x1,y1,x2,y2 = coord\n",
    "                maps[city]['t'][y1:y2,x1:x2] = t\n",
    "                maps[city]['p'][y1:y2,x1:x2] = p\n",
    "            for city in tqdm(maps.keys()):\n",
    "                maps[city]['fscore'] = metrics.f1_score(maps[city]['t'].flatten(), maps[city]['p'].flatten())\n",
    "            micro_f1 = metrics.f1_score(\n",
    "                np.concatenate([maps[city]['t'].flatten() for city in maps]),\n",
    "                np.concatenate([maps[city]['p'].flatten() for city in maps]), \n",
    "            )\n",
    "            macro_f1 = np.mean([maps[city]['fscore'] for city in maps]) \n",
    "            \n",
    "        print(checkpoint_path, scale)\n",
    "        print(macro_f1, micro_f1)\n",
    "    \n",
    "        results[checkpoint_path][scale] = {\n",
    "            'maps': maps,\n",
    "            'micro_f1': micro_f1,\n",
    "            'macro_f1': macro_f1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c63fd6-4241-4126-bfaf-479aff905247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/home/ani/change_detection.pytorch/checkpoints/levir_ibot_scale_aug/best_model.pth\n",
      "1x macro F1 = 0.355  micro-F1 = 0.898\n",
      "2x macro F1 = 0.354  micro-F1 = 0.897\n",
      "4x macro F1 = 0.352  micro-F1 = 0.892\n",
      "8x macro F1 = 0.342  micro-F1 = 0.875\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)\n",
    "    for scale in scales:\n",
    "        print(f\"{scale} macro F1 = {results[checkpoint][scale]['macro_f1']:.3f}  micro-F1 = {results[checkpoint][scale]['micro_f1']:.3f}\")\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59964303-3006-41fa-939f-eac981d3f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)\n",
    "    for scale in ['8x', '4x', '2x', '1x']:\n",
    "        print(f\",{results[checkpoint][scale]['micro_f1']:.3f}\", end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00adb878-d551-45aa-9e1d-421c30a465fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_sqrt (x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a65dc04-eff5-4c27-ac8f-acd03911d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ret_sqrt(5), ret_sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba248f7c-b5ad-4503-a1df-03de26c78714",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ret_sqrt(5)\n",
    "b= ret_sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0577b5-ba96-4dc7-8b15-a8adaf0274a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = c, b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c157b83-1e29-481c-b5e6-3c711ee5f83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767d852-4d7c-451b-8392-30a03a399d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
